A good start would be tokenizing dialectics. This is mostly a linguistic problem, but should be fun. Give user opportunity to alter ambigious tokens, or report mis-tokenization. Choice of default token is an interrestign machine learning problem. Tokens can come in several packages, for instance english (dictionary) and mathematics.

A better start is to jump right into everything. Use custom data structures to track holes in a json-schema, and make the user answer questions which fill these gaps.
